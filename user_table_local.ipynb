{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b1ae468-9b70-4587-a970-fb78666b4b88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAVA_HOME: C:\\Program Files\\Java\\jdk-17\n",
      "HADOOP_HOME: C:\\hadoop\n",
      "Initializing SparkSession... (This may take a few seconds)\n",
      "SparkSession created successfully!\n",
      "Attempting to read data from local path: d:\\study\\databricks\\data_cleaning\\local_environment\\test_data\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# --- WINDOWS ENVIRONMENT SETUP ---\n",
    "# VS Code might not pick up environment variables set in the terminal immediately.\n",
    "# We force set them here to ensure Spark works.\n",
    "\n",
    "# 1. Set HADOOP_HOME if missing\n",
    "if 'HADOOP_HOME' not in os.environ:\n",
    "    os.environ['HADOOP_HOME'] = r\"C:\\hadoop\"\n",
    "\n",
    "# 2. Add hadoop\\bin to PATH if missing\n",
    "if r\"hadoop\\bin\" not in os.environ['PATH']:\n",
    "    os.environ['PATH'] += \";\" + os.path.join(os.environ['HADOOP_HOME'], 'bin')\n",
    "\n",
    "# 3. FORCE SET JAVA_HOME to JDK 17 (Required for Spark 4.0)\n",
    "# Replace this path if your JDK 17 is installed elsewhere\n",
    "os.environ['JAVA_HOME'] = r\"C:\\Program Files\\Java\\jdk-17\" \n",
    "# Also update PATH to include this Java version\n",
    "os.environ['PATH'] = os.path.join(os.environ['JAVA_HOME'], 'bin') + \";\" + os.environ['PATH']\n",
    "\n",
    "print(f\"JAVA_HOME: {os.environ.get('JAVA_HOME')}\")\n",
    "print(f\"HADOOP_HOME: {os.environ.get('HADOOP_HOME')}\")\n",
    "\n",
    "# --- INITIALIZE SPARK ---\n",
    "print(\"Initializing SparkSession... (This may take a few seconds)\")\n",
    "\n",
    "# Using explicit config helps avoid Windows-specific hangs\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"LocalPipeline\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.host\", \"127.0.0.1\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", os.path.abspath(\"spark-warehouse\")) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"SparkSession created successfully!\")\n",
    "\n",
    "# COMMAND ----------\n",
    "# 1. Configuration: Read from local 'test_data' folder\n",
    "# Note: This runs locally on your machine, not on the Databricks cluster.\n",
    "\n",
    "LOCAL_FOLDER_NAME = \"test_data\"\n",
    "FILE_FORMAT       = \"json\"\n",
    "\n",
    "# Construct the path. Assuming test_data is in the same directory as this notebook.\n",
    "current_dir = os.getcwd()\n",
    "local_path = os.path.join(current_dir, LOCAL_FOLDER_NAME)\n",
    "\n",
    "print(f\"Attempting to read data from local path: {local_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimal_partition_col(df, candidates, target_partitions=20):\n",
    "    \"\"\"\n",
    "    Selects the best partition column based on cardinality.\n",
    "    For 1M rows, we aim for ~20 partitions to avoid small files.\n",
    "    \"\"\"\n",
    "    best_col = None\n",
    "    best_diff = float('inf')    \n",
    "    for col in candidates:\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "            \n",
    "        cardinality = df.select(col).distinct().count()        \n",
    "        if 1 < cardinality <= 100:\n",
    "            diff = abs(cardinality - target_partitions)\n",
    "            if diff < best_diff:\n",
    "                best_diff = diff\n",
    "                best_col = col\n",
    "                \n",
    "    return best_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['address',\n",
       " 'cart',\n",
       " 'category',\n",
       " 'order',\n",
       " 'order_item',\n",
       " 'payment',\n",
       " 'product',\n",
       " 'products_sku',\n",
       " 'subcategory',\n",
       " 'user',\n",
       " 'wishlist']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "# 2. Reading Datajects\n",
    "df_raw = spark.read.format(FILE_FORMAT).option(\"multiline\", \"true\").load(local_path)\n",
    "df_raw.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+-----------------------------+--------------------------+--------------------------+-----------------------+--------------------------------------------+-----------------------------------------------------+----------+---------------------+-----------------+---+--------------+\n",
      "|age|birth_of_date|company                      |create_time               |delete_time               |email                  |id                                          |job                                                  |password  |phone_number         |real_name        |sex|username      |\n",
      "+---+-------------+-----------------------------+--------------------------+--------------------------+-----------------------+--------------------------------------------+-----------------------------------------------------+----------+---------------------+-----------------+---+--------------+\n",
      "|67 |1958-11-27   |Mcknight-Guzman              |2025-11-29 11:57:14.220683|2025-11-29 11:57:14.862795|katieperry@gmail.com   |user_id-5c068bb5-36ba-481c-97d8-d5299688b225|Community development worker                         |$^w!8I$aF#|671-426-8034x9357    |Jessica Hernandez|F  |troy23        |\n",
      "|29 |1996-08-06   |Reynolds-Perez               |2025-11-29 11:57:14.221691|2025-11-29 11:57:14.845112|amandabrown@hotmail.com|user_id-13671dbb-abb5-4acb-92d9-92dc10b41e27|Armed forces logistics/support/administrative officer|@YQYu5ww86|001-557-447-2599x5389|David Davis      |M  |reyeschristina|\n",
      "|65 |1960-09-07   |Jackson, Walsh and Robertson |2025-11-29 11:57:14.222617|2025-11-29 11:57:14.907385|yvazquez@gmail.com     |user_id-82f6095a-2fda-446a-9d2c-bc41067332af|Engineer, petroleum                                  |*Z2H^l1r5z|+1-382-216-8261x8258 |Rhonda King      |F  |donna91       |\n",
      "|36 |1989-09-05   |Mason, Austin and Blankenship|2025-11-29 11:57:14.222617|2025-11-29 11:57:14.405463|adammcdonald@yahoo.com |user_id-71ea72d5-79f5-4364-9853-0343194d47d1|Metallurgist                                         |P@Z210qk8G|907-560-0245         |Tabitha Johnson  |F  |uevans        |\n",
      "|47 |1978-01-01   |Woods-Morales                |2025-11-29 11:57:14.222617|2025-11-29 11:57:14.712006|kendra62@gmail.com     |user_id-7372cc35-12bc-49dd-a9ca-0fcf52c5ac94|Adult guidance worker                                |$q+8lRl5$L|+1-898-221-2374x7525 |Timothy Jackson  |M  |mitchell06    |\n",
      "+---+-------------+-----------------------------+--------------------------+--------------------------+-----------------------+--------------------------------------------+-----------------------------------------------------+----------+---------------------+-----------------+---+--------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "\n",
    "user_table = df_raw.select(\"user.*\")\n",
    "user_table.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_table = df_raw.select(\"user.*\")\n",
    "user_table = user_table.withColumn(\"real_name\", F.trim(F.lower(F.col(\"real_name\"))))\n",
    "user_table = user_table.withColumn(\"company\", F.trim(F.lower(F.col(\"company\"))))\n",
    "user_table = user_table.withColumn(\"job\", F.trim(F.lower(F.col(\"job\"))))\n",
    "\n",
    "\n",
    "window_spec = Window.partitionBy(\"real_name\").orderBy(\"real_name\")\n",
    "df_with_row_number = user_table.withColumn(\"row_number\", F.row_number().over(window_spec))\n",
    "df_cleaned = df_with_row_number.filter(F.col(\"row_number\") == 1).select(\n",
    "    \"real_name\",\n",
    "    \"age\",\n",
    "    \"birth_of_date\",\n",
    "    \"company\",\n",
    "    \"job\",\n",
    "    \"phone_number\",\n",
    "    \"sex\"\n",
    ")\n",
    "\n",
    "updated_df = user_table.alias(\"raw\").join(\n",
    "    df_cleaned.alias(\"cleaned\"),\n",
    "    on=F.col(\"raw.real_name\") == F.col(\"cleaned.real_name\"),\n",
    "    how=\"left\"\n",
    ")\n",
    "cleaned_columns = df_cleaned.columns\n",
    "raw_columns = user_table.columns\n",
    "raw_only_columns = [col for col in raw_columns if col not in cleaned_columns]\n",
    "df_cleaned = updated_df.select(\n",
    "    *[F.col(f\"cleaned.{col}\").alias(col) for col in cleaned_columns] +  \n",
    "    [F.col(f\"raw.{col}\").alias(col) for col in raw_only_columns]        \n",
    ")\n",
    "\n",
    "candidates = [\"sex\", \"job\", \"company\", \"age\", \"country_code\"]\n",
    "selected_partition_col = get_optimal_partition_col(df_cleaned, candidates, target_partitions=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sex'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_partition_col"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "testing pipeline",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": ".local_venv",
   "language": "python",
   "name": "local_spark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
