{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eddf279c-e3fa-455b-9b4d-3cff4a17d095",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook parameters - these will be passed from ADF\n",
    "dbutils.widgets.text(\"user_id\", \"\")\n",
    "dbutils.widgets.text(\"parent_job_id\", \"\")\n",
    "entity_type = \"order_item\" # Default to order_item for this notebook\n",
    "\n",
    "# Get parameters\n",
    "user_id = dbutils.widgets.get(\"user_id\")\n",
    "parent_job_id = dbutils.widgets.get(\"parent_job_id\")\n",
    "\n",
    "\n",
    "# Validate required parameters\n",
    "if not user_id or not parent_job_id:\n",
    "    raise ValueError(\"user_id and parent_job_id are required parameters\")\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import IntegerType, StringType, TimestampType\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "def get_optimal_partition_col(df, candidates):\n",
    "    \"\"\"\n",
    "    Selects the best partition column based on cardinality.\n",
    "    Dynamically calculates target partitions based on data size for optimal performance.\n",
    "    \"\"\"\n",
    "    num_rows = df.count()\n",
    "    target_partitions = max(4, min(50, num_rows // 20000))  # Adjust divisor as needed based on row size\n",
    "    \n",
    "    best_col = None\n",
    "    best_diff = float('inf')    \n",
    "    for col in candidates:\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "            \n",
    "        cardinality = df.select(col).distinct().count()        \n",
    "        if 1 < cardinality <= 100:\n",
    "            diff = abs(cardinality - target_partitions)\n",
    "            if diff < best_diff:\n",
    "                best_diff = diff\n",
    "                best_col = col\n",
    "                \n",
    "    return best_col\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "from pyspark.sql.functions import col, current_timestamp, lit, lower, trim, regexp_replace, to_timestamp\n",
    "\n",
    "# Dynamic configuration based on parameters\n",
    "ADLS_ACCOUNT_NAME = \"shanleestorage\"  # Your storage account name\n",
    "\n",
    "# Bronze layer paths (source)\n",
    "RAW_CONTAINER = \"shanlee-raw-data\"\n",
    "RAW_DATA_PATH = f\"{user_id}/{parent_job_id}\"  # Dynamic path based on user/batch\n",
    "RAW_FULL_PATH = f\"abfss://{RAW_CONTAINER}@{ADLS_ACCOUNT_NAME}.dfs.core.windows.net/{RAW_DATA_PATH}\"\n",
    "\n",
    "# Silver layer paths (destination)\n",
    "SILVER_CONTAINER = \"shanlee-cleaned-data\"\n",
    "SILVER_PATH = f\"temp_spark/{user_id}/{parent_job_id}/{entity_type}\"\n",
    "SILVER_FULL_PATH = f\"abfss://{SILVER_CONTAINER}@{ADLS_ACCOUNT_NAME}.dfs.core.windows.net/{SILVER_PATH}\"\n",
    "\n",
    "print(f\"Reading from: {RAW_FULL_PATH}\")\n",
    "print(f\"Writing to: {SILVER_FULL_PATH}\")\n",
    "\n",
    "# Authentication (same as before)\n",
    "SECRET_SCOPE = \"AdlsAccessKey\"    \n",
    "SECRET_KEY = \"AdlsAccessKey\"\n",
    "\n",
    "try:\n",
    "    access_key_value = dbutils.secrets.get(scope=SECRET_SCOPE, key=SECRET_KEY)\n",
    "    \n",
    "    spark.conf.set(\n",
    "        f\"fs.azure.account.key.{ADLS_ACCOUNT_NAME}.dfs.core.windows.net\",\n",
    "        access_key_value\n",
    "    )\n",
    "    \n",
    "    print(\"Authentication successful: Spark configured to access ADLS Gen2.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"FATAL ERROR: Could not retrieve secret. Check scope/key names. Error: {e}\")\n",
    "    dbutils.notebook.exit(\"Authentication Failed\")\n",
    "\n",
    "# Read raw data from Bronze layer\n",
    "df_raw = spark.read.format(\"json\") \\\n",
    "             .option(\"multiline\", \"true\") \\\n",
    "             .load(RAW_FULL_PATH)\n",
    "order_item_table = df_raw.select(\"order_item.*\")\n",
    "\n",
    "# Global encoding cleanup: Remove non-printable characters from all string columns\n",
    "for column in order_item_table.columns:\n",
    "    if dict(order_item_table.dtypes)[column] == 'string':\n",
    "        order_item_table = order_item_table.withColumn(\n",
    "            column, \n",
    "            F.regexp_replace(F.col(column), '[^\\\\x20-\\\\x7E]', '')\n",
    "        )\n",
    "\n",
    "# Filter out rows containing 'invalid' in any column\n",
    "for column in order_item_table.columns:\n",
    "    order_item_table = order_item_table.filter(~F.lower(F.col(column).cast(\"string\")).contains(\"invalid\"))\n",
    "\n",
    "# Remove all rows with any null values in any column (BEFORE type casting)\n",
    "order_item_table = order_item_table.dropna()\n",
    "\n",
    "# Explicit type casting for all ID columns to StringType (BEFORE quantity to avoid conflicts)\n",
    "id_columns = [column for column in order_item_table.columns if 'id' in column.lower()]\n",
    "for column in id_columns:\n",
    "    order_item_table = order_item_table.withColumn(column, F.col(column).cast(StringType()))\n",
    "\n",
    "# Clean quantity column: cast to IntegerType and limit to 0 <= quantity < 10000\n",
    "order_item_table = order_item_table.withColumn(\"quantity\", F.col(\"quantity\").cast(IntegerType())) \\\n",
    "    .filter(F.col(\"quantity\").isNotNull() & (F.col(\"quantity\") >= 0) & (F.col(\"quantity\") < 10000))\n",
    "\n",
    "# Explicit timestamp casting for datetime columns\n",
    "timestamp_columns = ['create_time', 'updated_at']\n",
    "for column in timestamp_columns:\n",
    "    if column in order_item_table.columns:\n",
    "        order_item_table = order_item_table.withColumn(\n",
    "            column,\n",
    "            to_timestamp(F.col(column))\n",
    "        )\n",
    "\n",
    "# COMMAND ----------\n",
    "# --- 5. LOAD TO SILVER LAYER: Write Cleaned Data as Delta Lake ---\n",
    "\n",
    "# Use Delta format for reliability, transactions, and schema enforcement.\n",
    "\n",
    "writer = order_item_table.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "\n",
    "writer.save(SILVER_FULL_PATH)\n",
    "\n",
    "print(f\"Processed {order_item_table.count()} cleaned records\")\n",
    "print(\"Next Steps: Gold layer processing will handle aggregations and joins.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "order_item_table",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
