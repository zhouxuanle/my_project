{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e78e5935-722b-41d2-afc1-7810236e5d5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook parameters - these will be passed from ADF\n",
    "dbutils.widgets.text(\"user_id\", \"\")\n",
    "dbutils.widgets.text(\"parent_job_id\", \"\")\n",
    "entity_type = \"products_sku\"  # Default to products_sku for this notebook\n",
    "\n",
    "# Get parameters\n",
    "user_id = dbutils.widgets.get(\"user_id\")\n",
    "parent_job_id = dbutils.widgets.get(\"parent_job_id\")\n",
    "\n",
    "\n",
    "# Validate required parameters\n",
    "if not user_id or not parent_job_id:\n",
    "    raise ValueError(\"user_id and parent_job_id are required parameters\")\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import IntegerType, DoubleType, StringType, TimestampType\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "def get_optimal_partition_col(df, candidates):\n",
    "    \"\"\"\n",
    "    Selects the best partition column based on cardinality.\n",
    "    Dynamically calculates target partitions based on data size for optimal performance.\n",
    "    \"\"\"\n",
    "    num_rows = df.count()\n",
    "    target_partitions = max(4, min(50, num_rows // 20000))  # Adjust divisor as needed based on row size\n",
    "    \n",
    "    best_col = None\n",
    "    best_diff = float('inf')    \n",
    "    for col in candidates:\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "            \n",
    "        cardinality = df.select(col).distinct().count()        \n",
    "        if 1 < cardinality <= 100:\n",
    "            diff = abs(cardinality - target_partitions)\n",
    "            if diff < best_diff:\n",
    "                best_diff = diff\n",
    "                best_col = col\n",
    "                \n",
    "    return best_col\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "from pyspark.sql.functions import col, current_timestamp, lit, lower, trim, regexp_replace, to_timestamp\n",
    "\n",
    "# Dynamic configuration based on parameters\n",
    "ADLS_ACCOUNT_NAME = \"shanleestorage\"  # Your storage account name\n",
    "\n",
    "# Bronze layer paths (source)\n",
    "RAW_CONTAINER = \"shanlee-raw-data\"\n",
    "RAW_DATA_PATH = f\"{user_id}/{parent_job_id}\"  # Dynamic path based on user/batch\n",
    "RAW_FULL_PATH = f\"abfss://{RAW_CONTAINER}@{ADLS_ACCOUNT_NAME}.dfs.core.windows.net/{RAW_DATA_PATH}\"\n",
    "\n",
    "# Silver layer paths (destination)\n",
    "SILVER_CONTAINER = \"shanlee-cleaned-data\"\n",
    "SILVER_PATH = f\"temp_spark/{user_id}/{parent_job_id}/{entity_type}\"\n",
    "SILVER_FULL_PATH = f\"abfss://{SILVER_CONTAINER}@{ADLS_ACCOUNT_NAME}.dfs.core.windows.net/{SILVER_PATH}\"\n",
    "\n",
    "print(f\"Reading from: {RAW_FULL_PATH}\")\n",
    "print(f\"Writing to: {SILVER_FULL_PATH}\")\n",
    "\n",
    "# Authentication (same as before)\n",
    "SECRET_SCOPE = \"AdlsAccessKey\"    \n",
    "SECRET_KEY = \"AdlsAccessKey\"\n",
    "\n",
    "try:\n",
    "    access_key_value = dbutils.secrets.get(scope=SECRET_SCOPE, key=SECRET_KEY)\n",
    "    \n",
    "    spark.conf.set(\n",
    "        f\"fs.azure.account.key.{ADLS_ACCOUNT_NAME}.dfs.core.windows.net\",\n",
    "        access_key_value\n",
    "    )\n",
    "    \n",
    "    print(\"Authentication successful: Spark configured to access ADLS Gen2.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"FATAL ERROR: Could not retrieve secret. Check scope/key names. Error: {e}\")\n",
    "    dbutils.notebook.exit(\"Authentication Failed\")\n",
    "\n",
    "# Read raw data from Bronze layer\n",
    "df_raw = spark.read.format(\"json\") \\\n",
    "             .option(\"multiline\", \"true\") \\\n",
    "             .load(RAW_FULL_PATH)\n",
    "products_sku_table = df_raw.select(\"products_sku.*\")\n",
    "\n",
    "# Global encoding cleanup: Remove non-printable characters from all string columns\n",
    "for column in products_sku_table.columns:\n",
    "    if dict(products_sku_table.dtypes)[column] == 'string':\n",
    "        products_sku_table = products_sku_table.withColumn(\n",
    "            column, \n",
    "            F.regexp_replace(F.col(column), '[^\\\\x20-\\\\x7E]', '')\n",
    "        )\n",
    "\n",
    "# Filter out rows containing 'invalid' in any column\n",
    "for column in products_sku_table.columns:\n",
    "    products_sku_table = products_sku_table.filter(~F.lower(F.col(column).cast(\"string\")).contains(\"invalid\"))\n",
    "\n",
    "# Remove all rows with any null values in any column (BEFORE type casting)\n",
    "products_sku_table = products_sku_table.dropna()\n",
    "\n",
    "# Explicit type casting for all ID columns to StringType (BEFORE numeric columns)\n",
    "id_columns = [column for column in products_sku_table.columns if 'id' in column.lower()]\n",
    "for column in id_columns:\n",
    "    products_sku_table = products_sku_table.withColumn(column, F.col(column).cast(StringType()))\n",
    "\n",
    "# Cast quantity to IntegerType, filter 0-9999\n",
    "products_sku_table = products_sku_table.withColumn(\"quantity\", F.col(\"quantity\").cast(IntegerType())) \\\n",
    "                                       .filter((F.col(\"quantity\") >= 0) & (F.col(\"quantity\") < 10000))\n",
    "\n",
    "# Filter price DoubleType 0-9999, cast to DoubleType\n",
    "products_sku_table = products_sku_table.filter((F.col(\"price\").cast(DoubleType()) >= 0) & (F.col(\"price\").cast(DoubleType()) < 10000)) \\\n",
    "                                       .withColumn(\"price\", F.col(\"price\").cast(DoubleType()))\n",
    "\n",
    "# Explicit timestamp casting for datetime columns\n",
    "timestamp_columns = ['create_time', 'delete_time']\n",
    "for column in timestamp_columns:\n",
    "    if column in products_sku_table.columns:\n",
    "        products_sku_table = products_sku_table.withColumn(\n",
    "            column,\n",
    "            to_timestamp(F.col(column))\n",
    "        )\n",
    "\n",
    "# Remove duplicates based on id, keeping any one row\n",
    "products_sku_table = products_sku_table.dropDuplicates([\"id\"])\n",
    "\n",
    "# COMMAND ----------\n",
    "# --- 5. LOAD TO SILVER LAYER: Write Cleaned Data as Delta Lake ---\n",
    "\n",
    "# Use Delta format for reliability, transactions, and schema enforcement.\n",
    "\n",
    "writer = products_sku_table.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "\n",
    "writer.save(SILVER_FULL_PATH)\n",
    "\n",
    "print(f\"Processed {products_sku_table.count()} cleaned records\")\n",
    "print(\"Next Steps: Gold layer processing will handle aggregations and joins.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "products_sku_table",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
