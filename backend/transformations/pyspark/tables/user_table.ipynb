{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d42c295-fe50-4d61-a922-96f03d03b354",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook parameters - these will be passed from ADF\n",
    "dbutils.widgets.text(\"user_id\", \"\")\n",
    "dbutils.widgets.text(\"parent_job_id\", \"\")\n",
    "entity_type = \"user\"  # Default to user for this notebook\n",
    "\n",
    "# Get parameters\n",
    "user_id = dbutils.widgets.get(\"user_id\")\n",
    "parent_job_id = dbutils.widgets.get(\"parent_job_id\")\n",
    "\n",
    "\n",
    "\n",
    "# Validate required parameters\n",
    "if not user_id or not parent_job_id:\n",
    "    raise ValueError(\"user_id and parent_job_id are required parameters\")\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "def get_optimal_partition_col(df, candidates):\n",
    "    \"\"\"\n",
    "    Selects the best partition column based on cardinality.\n",
    "    Dynamically calculates target partitions based on data size for optimal performance.\n",
    "    \"\"\"\n",
    "    num_rows = df.count()\n",
    "    target_partitions = max(4, min(50, num_rows // 20000))  # Adjust divisor as needed based on row size\n",
    "    \n",
    "    best_col = None\n",
    "    best_diff = float('inf')    \n",
    "    for col in candidates:\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "            \n",
    "        cardinality = df.select(col).distinct().count()        \n",
    "        if 1 < cardinality <= 100:\n",
    "            diff = abs(cardinality - target_partitions)\n",
    "            if diff < best_diff:\n",
    "                best_diff = diff\n",
    "                best_col = col\n",
    "                \n",
    "    return best_col\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "from pyspark.sql.functions import col, current_timestamp, lit, lower, trim, regexp_replace\n",
    "\n",
    "# Dynamic configuration based on parameters\n",
    "ADLS_ACCOUNT_NAME = \"shanleestorage\"  # Your storage account name\n",
    "\n",
    "# Bronze layer paths (source)\n",
    "RAW_CONTAINER = \"shanlee-raw-data\"\n",
    "RAW_DATA_PATH = f\"{user_id}/{parent_job_id}\"  # Dynamic path based on user/batch\n",
    "RAW_FULL_PATH = f\"abfss://{RAW_CONTAINER}@{ADLS_ACCOUNT_NAME}.dfs.core.windows.net/{RAW_DATA_PATH}\"\n",
    "\n",
    "# Silver layer paths (destination)\n",
    "SILVER_CONTAINER = \"shanlee-cleaned-data\"\n",
    "SILVER_PATH = f\"temp_spark/{user_id}/{parent_job_id}/{entity_type}\"\n",
    "SILVER_FULL_PATH = f\"abfss://{SILVER_CONTAINER}@{ADLS_ACCOUNT_NAME}.dfs.core.windows.net/{SILVER_PATH}\"\n",
    "\n",
    "print(f\"Reading from: {RAW_FULL_PATH}\")\n",
    "print(f\"Writing to: {SILVER_FULL_PATH}\")\n",
    "\n",
    "# Authentication (same as before)\n",
    "SECRET_SCOPE = \"AdlsAccessKey\"    \n",
    "SECRET_KEY = \"AdlsAccessKey\"\n",
    "\n",
    "try:\n",
    "    access_key_value = dbutils.secrets.get(scope=SECRET_SCOPE, key=SECRET_KEY)\n",
    "    \n",
    "    spark.conf.set(\n",
    "        f\"fs.azure.account.key.{ADLS_ACCOUNT_NAME}.dfs.core.windows.net\",\n",
    "        access_key_value\n",
    "    )\n",
    "    \n",
    "    print(\"Authentication successful: Spark configured to access ADLS Gen2.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"FATAL ERROR: Could not retrieve secret. Check scope/key names. Error: {e}\")\n",
    "    dbutils.notebook.exit(\"Authentication Failed\")\n",
    "\n",
    "# Read raw data from Bronze layer\n",
    "df_raw = spark.read.format(\"json\") \\\n",
    "             .option(\"multiline\", \"true\") \\\n",
    "             .load(RAW_FULL_PATH)\n",
    "user_table = df_raw.select(\"user.*\")\n",
    "\n",
    "# Global encoding cleanup: Remove non-printable characters from all string columns\n",
    "for column in user_table.columns:\n",
    "    if dict(user_table.dtypes)[column] == 'string':\n",
    "        user_table = user_table.withColumn(\n",
    "            column, \n",
    "            F.regexp_replace(F.col(column), '[^\\\\x20-\\\\x7E]', '')\n",
    "        )\n",
    "\n",
    "user_table = user_table.withColumn(\"real_name\", F.trim(F.lower(F.col(\"real_name\"))))\n",
    "user_table = user_table.withColumn(\"company\", F.trim(F.lower(F.col(\"company\"))))\n",
    "user_table = user_table.withColumn(\"job\", F.trim(F.lower(F.col(\"job\"))))\n",
    "user_table = user_table.dropDuplicates([\"id\"])\n",
    "\n",
    "# Deduplicate by user_id (using standard dropDuplicates which keeps first occurrence by default)\n",
    "# Note: PySpark doesn't have a 'keep' parameter like Pandas, so we can't specify 'last'\n",
    "# To simulate 'keep=last', we would need to add row_number() logic, but for simplicity, keeping first\n",
    "user_table = user_table.dropDuplicates([\"user_id\"])\n",
    "\n",
    "# Filter out rows containing 'invalid' in any column\n",
    "for column in user_table.columns:\n",
    "    user_table = user_table.filter(~F.lower(F.col(column).cast(\"string\")).contains(\"invalid\"))\n",
    "\n",
    "# Filter sex: ensure it's one of 'male', 'female', or 'other' (case-insensitive, trimmed)\n",
    "user_table = user_table.filter(F.lower(F.trim(F.col(\"sex\"))).isin([\"male\", \"female\", \"other\"]))\n",
    "\n",
    "# Filter age: ensure it's numeric and within valid range (10-100)\n",
    "user_table = user_table.withColumn(\"age\", F.col(\"age\").cast(\"int\")) \\\n",
    "    .filter((F.col(\"age\") >= 0) & (F.col(\"age\") <= 100))\n",
    "\n",
    "# Filter birth_of_date: ensure it's a valid date and less than today\n",
    "user_table = user_table.withColumn(\"birth_of_date\", F.to_date(F.col(\"birth_of_date\"))) \\\n",
    "    .filter(F.col(\"birth_of_date\") < F.current_date())\n",
    "\n",
    "# Remove all rows with any null values (AFTER type casting to handle coercion nulls)\n",
    "user_table = user_table.dropna()\n",
    "\n",
    "# Remove duplicates based on real_name\n",
    "user_table = user_table.dropDuplicates([\"real_name\"])\n",
    "\n",
    "# COMMAND ----------\n",
    "# --- 5. LOAD TO SILVER LAYER: Write Cleaned Data as Delta Lake ---\n",
    "\n",
    "# Use Delta format for reliability, transactions, and schema enforcement.\n",
    "\n",
    "writer = user_table.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "\n",
    "writer.save(SILVER_FULL_PATH)\n",
    "\n",
    "print(f\"Processed {user_table.count()} cleaned records\")\n",
    "print(\"Next Steps: Verify data in ADLS Gen2 and proceed with Synapse loading (Step 6b).\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "user_table",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
