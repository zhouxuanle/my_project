{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4e6922c7-0a09-41a7-80d2-b373c69d7c58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook parameters - these will be passed from ADF\n",
    "dbutils.widgets.text(\"user_id\", \"\")\n",
    "dbutils.widgets.text(\"parent_job_id\", \"\")\n",
    "entity_type =  \"subcategory\"  # Default to subcategory for this notebook\n",
    "\n",
    "# Get parameters\n",
    "user_id = dbutils.widgets.get(\"user_id\")\n",
    "parent_job_id = dbutils.widgets.get(\"parent_job_id\")\n",
    "\n",
    "\n",
    "\n",
    "# Validate required parameters\n",
    "if not user_id or not parent_job_id:\n",
    "    raise ValueError(\"user_id and parent_job_id are required parameters\")\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "def get_optimal_partition_col(df, candidates):\n",
    "    \"\"\"\n",
    "    Selects the best partition column based on cardinality.\n",
    "    Dynamically calculates target partitions based on data size for optimal performance.\n",
    "    \"\"\"\n",
    "    num_rows = df.count()\n",
    "    target_partitions = max(4, min(50, num_rows // 20000))  # Adjust divisor as needed based on row size\n",
    "    \n",
    "    best_col = None\n",
    "    best_diff = float('inf')    \n",
    "    for col in candidates:\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "            \n",
    "        cardinality = df.select(col).distinct().count()        \n",
    "        if 1 < cardinality <= 100:\n",
    "            diff = abs(cardinality - target_partitions)\n",
    "            if diff < best_diff:\n",
    "                best_diff = diff\n",
    "                best_col = col\n",
    "                \n",
    "    return best_col\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "from pyspark.sql.functions import col, current_timestamp, lit, lower, trim, regexp_replace, to_timestamp, when\n",
    "\n",
    "# Dynamic configuration based on parameters\n",
    "ADLS_ACCOUNT_NAME = \"shanleestorage\"  # Your storage account name\n",
    "\n",
    "# Bronze layer paths (source)\n",
    "RAW_CONTAINER = \"shanlee-raw-data\"\n",
    "RAW_DATA_PATH = f\"{user_id}/{parent_job_id}\"  # Dynamic path based on user/batch\n",
    "RAW_FULL_PATH = f\"abfss://{RAW_CONTAINER}@{ADLS_ACCOUNT_NAME}.dfs.core.windows.net/{RAW_DATA_PATH}\"\n",
    "\n",
    "# Silver layer paths (destination)\n",
    "SILVER_CONTAINER = \"shanlee-cleaned-data\"\n",
    "SILVER_PATH = f\"temp_spark/{user_id}/{parent_job_id}/{entity_type}\"\n",
    "SILVER_FULL_PATH = f\"abfss://{SILVER_CONTAINER}@{ADLS_ACCOUNT_NAME}.dfs.core.windows.net/{SILVER_PATH}\"\n",
    "\n",
    "print(f\"Reading from: {RAW_FULL_PATH}\")\n",
    "print(f\"Writing to: {SILVER_FULL_PATH}\")\n",
    "\n",
    "# Authentication (same as before)\n",
    "SECRET_SCOPE = \"AdlsAccessKey\"    \n",
    "SECRET_KEY = \"AdlsAccessKey\"\n",
    "\n",
    "try:\n",
    "    access_key_value = dbutils.secrets.get(scope=SECRET_SCOPE, key=SECRET_KEY)\n",
    "    \n",
    "    spark.conf.set(\n",
    "        f\"fs.azure.account.key.{ADLS_ACCOUNT_NAME}.dfs.core.windows.net\",\n",
    "        access_key_value\n",
    "    )\n",
    "    \n",
    "    print(\"Authentication successful: Spark configured to access ADLS Gen2.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"FATAL ERROR: Could not retrieve secret. Check scope/key names. Error: {e}\")\n",
    "    dbutils.notebook.exit(\"Authentication Failed\")\n",
    "\n",
    "# Read raw data from Bronze layer\n",
    "df_raw = spark.read.format(\"json\") \\\n",
    "             .option(\"multiline\", \"true\") \\\n",
    "             .load(RAW_FULL_PATH)\n",
    "subcategory_table = df_raw.select(\"subcategory.*\")\n",
    "\n",
    "# Global encoding cleanup: Remove non-printable characters from all string columns\n",
    "for column in subcategory_table.columns:\n",
    "    if dict(subcategory_table.dtypes)[column] == 'string':\n",
    "        subcategory_table = subcategory_table.withColumn(\n",
    "            column, \n",
    "            F.regexp_replace(F.col(column), '[^\\\\x20-\\\\x7E]', '')\n",
    "        )\n",
    "\n",
    "subcategory_table = subcategory_table.withColumn(\"description\", F.trim(F.lower(F.col(\"description\")))) \\\n",
    "                                     .withColumn(\"name\", F.trim(F.lower(F.col(\"name\")))) \n",
    "# Filter out rows containing 'invalid' in any column\n",
    "for column in subcategory_table.columns:\n",
    "    subcategory_table = subcategory_table.filter(~F.lower(F.col(column).cast(\"string\")).contains(\"invalid\"))\n",
    "\n",
    "# Filter future-dated timestamps: cap to current timestamp if future\n",
    "timestamp_columns = ['create_time', 'updated_at', 'delete_time']\n",
    "for column in timestamp_columns:\n",
    "    if column in subcategory_table.columns:\n",
    "        subcategory_table = subcategory_table.withColumn(\n",
    "            column,\n",
    "            to_timestamp(F.col(column))\n",
    "        ).withColumn(\n",
    "            column,\n",
    "            when(F.col(column) > current_timestamp(), current_timestamp()).otherwise(F.col(column))\n",
    "        )\n",
    "\n",
    "# Remove all rows with any null values (AFTER type casting to handle coercion nulls)\n",
    "subcategory_table = subcategory_table.dropna()\n",
    "\n",
    "# Remove duplicates based on id, keeping any one row\n",
    "subcategory_table = subcategory_table.dropDuplicates([\"id\"])\n",
    "\n",
    "# COMMAND ----------\n",
    "# --- 5. LOAD TO SILVER LAYER: Write Cleaned Data as Delta Lake ---\n",
    "\n",
    "# Use Delta format for reliability, transactions, and schema enforcement.\n",
    "\n",
    "writer = subcategory_table.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "\n",
    "writer.save(SILVER_FULL_PATH)\n",
    "\n",
    "print(f\"Processed {subcategory_table.count()} cleaned records\")\n",
    "print(\"Next Steps: Gold layer processing will handle aggregations and joins.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "subcategory_table",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
