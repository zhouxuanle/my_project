{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5cb662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Databricks notebook parameters - these will be passed from ADF\n",
    "dbutils.widgets.text(\"user_id\", \"\")\n",
    "dbutils.widgets.text(\"parent_job_id\", \"\")\n",
    "dbutils.widgets.text(\"entity_type\", \"user\")  # Default to user for this notebook\n",
    "\n",
    "# Get parameters\n",
    "user_id = dbutils.widgets.get(\"user_id\")\n",
    "parent_job_id = dbutils.widgets.get(\"parent_job_id\")\n",
    "entity_type = dbutils.widgets.get(\"entity_type\")\n",
    "\n",
    "print(f\"Processing {entity_type} data for user {user_id}, batch {parent_job_id}\")\n",
    "\n",
    "# Validate required parameters\n",
    "if not user_id or not parent_job_id:\n",
    "    raise ValueError(\"user_id and parent_job_id are required parameters\")\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "def get_optimal_partition_col(df, candidates):\n",
    "    \"\"\"\n",
    "    Selects the best partition column based on cardinality.\n",
    "    Dynamically calculates target partitions based on data size for optimal performance.\n",
    "    \"\"\"\n",
    "    num_rows = df.count()\n",
    "    target_partitions = max(4, min(50, num_rows // 20000))  # Adjust divisor as needed based on row size\n",
    "    \n",
    "    best_col = None\n",
    "    best_diff = float('inf')    \n",
    "    for col in candidates:\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "            \n",
    "        cardinality = df.select(col).distinct().count()        \n",
    "        if 1 < cardinality <= 100:\n",
    "            diff = abs(cardinality - target_partitions)\n",
    "            if diff < best_diff:\n",
    "                best_diff = diff\n",
    "                best_col = col\n",
    "                \n",
    "    return best_col\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "from pyspark.sql.functions import col, current_timestamp, lit, lower, trim, regexp_replace\n",
    "print(f'Starting {entity_type} table processing...')\n",
    "\n",
    "# Dynamic configuration based on parameters\n",
    "ADLS_ACCOUNT_NAME = \"shanleestorage\"  # Your storage account name\n",
    "\n",
    "# Bronze layer paths (source)\n",
    "RAW_CONTAINER = \"shanlee-raw-data\"\n",
    "RAW_DATA_PATH = f\"{user_id}/{parent_job_id}\"  # Dynamic path based on user/batch\n",
    "RAW_FULL_PATH = f\"abfss://{RAW_CONTAINER}@{ADLS_ACCOUNT_NAME}.dfs.core.windows.net/{RAW_DATA_PATH}\"\n",
    "\n",
    "# Silver layer paths (destination)\n",
    "SILVER_CONTAINER = \"shanlee-cleaned-data\"\n",
    "SILVER_PATH = f\"silver/cleaned/{user_id}/{parent_job_id}/{entity_type}\"\n",
    "SILVER_FULL_PATH = f\"abfss://{SILVER_CONTAINER}@{ADLS_ACCOUNT_NAME}.dfs.core.windows.net/{SILVER_PATH}\"\n",
    "\n",
    "print(f\"Reading from: {RAW_FULL_PATH}\")\n",
    "print(f\"Writing to: {SILVER_FULL_PATH}\")\n",
    "\n",
    "# Authentication (same as before)\n",
    "SECRET_SCOPE = \"AdlsAccessKey\"    \n",
    "SECRET_KEY = \"AdlsAccessKey\"\n",
    "\n",
    "try:\n",
    "    access_key_value = dbutils.secrets.get(scope=SECRET_SCOPE, key=SECRET_KEY)\n",
    "    \n",
    "    spark.conf.set(\n",
    "        f\"fs.azure.account.key.{ADLS_ACCOUNT_NAME}.dfs.core.windows.net\",\n",
    "        access_key_value\n",
    "    )\n",
    "    \n",
    "    print(\"Authentication successful: Spark configured to access ADLS Gen2.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"FATAL ERROR: Could not retrieve secret. Check scope/key names. Error: {e}\")\n",
    "    dbutils.notebook.exit(\"Authentication Failed\")\n",
    "\n",
    "# Read raw data from Bronze layer\n",
    "df_raw = spark.read.format(\"json\") \\\n",
    "             .option(\"multiline\", \"true\") \\\n",
    "             .load(RAW_FULL_PATH)\n",
    "\n",
    "# Extract the specific entity table from the raw data\n",
    "entity_table = df_raw.select(f\"{entity_type}.*\")\n",
    "\n",
    "print(f\"Loaded {entity_table.count()} raw {entity_type} records\")\n",
    "\n",
    "# Apply entity-specific transformations\n",
    "if entity_type == \"user\":\n",
    "    # User-specific cleaning logic\n",
    "    entity_table = entity_table.withColumn(\"real_name\", F.trim(F.lower(F.col(\"real_name\"))))\n",
    "    entity_table = entity_table.withColumn(\"company\", F.trim(F.lower(F.col(\"company\"))))\n",
    "    entity_table = entity_table.withColumn(\"job\", F.trim(F.lower(F.col(\"job\"))))\n",
    "    entity_table = entity_table.dropDuplicates([\"id\"])\n",
    "\n",
    "    # Filter out rows containing 'invalid' in any column\n",
    "    for column in entity_table.columns:\n",
    "        entity_table = entity_table.filter(~F.lower(F.col(column).cast(\"string\")).contains(\"invalid\"))\n",
    "\n",
    "    # Filter sex: ensure it's one of 'male', 'female', or 'other' (case-insensitive, trimmed)\n",
    "    entity_table = entity_table.filter(F.lower(F.trim(F.col(\"sex\"))).isin([\"male\", \"female\", \"other\"]))\n",
    "\n",
    "    # Filter age: ensure it's numeric and within valid range (10-100)\n",
    "    entity_table = entity_table.withColumn(\"age\", F.col(\"age\").cast(\"int\")) \\\n",
    "        .filter(F.col(\"age\").isNotNull() & (F.col(\"age\") >= 10) & (F.col(\"age\") <= 100))\n",
    "\n",
    "    # Filter birth_of_date: ensure it's a valid date and less than today\n",
    "    entity_table = entity_table.withColumn(\"birth_of_date\", F.to_date(F.col(\"birth_of_date\"))) \\\n",
    "        .filter(F.col(\"birth_of_date\").isNotNull() & (F.col(\"birth_of_date\") < F.current_date()))\n",
    "\n",
    "    # Remove duplicates based on real_name\n",
    "    entity_table = entity_table.dropDuplicates([\"real_name\"])\n",
    "\n",
    "elif entity_type == \"address\":\n",
    "    # Address-specific cleaning logic\n",
    "    entity_table = entity_table.withColumn(\"street\", F.trim(F.lower(F.col(\"street\"))))\n",
    "    entity_table = entity_table.withColumn(\"city\", F.trim(F.lower(F.col(\"city\"))))\n",
    "    entity_table = entity_table.withColumn(\"country\", F.trim(F.lower(F.col(\"country\"))))\n",
    "    entity_table = entity_table.dropDuplicates([\"id\"])\n",
    "    \n",
    "    # Add your address validation logic here\n",
    "    \n",
    "elif entity_type == \"product\":\n",
    "    # Product-specific cleaning logic\n",
    "    entity_table = entity_table.withColumn(\"name\", F.trim(F.lower(F.col(\"name\"))))\n",
    "    entity_table = entity_table.withColumn(\"category\", F.trim(F.lower(F.col(\"category\"))))\n",
    "    entity_table = entity_table.dropDuplicates([\"id\"])\n",
    "    \n",
    "    # Add your product validation logic here\n",
    "\n",
    "# Add more entity types as needed...\n",
    "\n",
    "else:\n",
    "    print(f\"Warning: No specific cleaning logic defined for entity type '{entity_type}'. Applying basic deduplication only.\")\n",
    "    entity_table = entity_table.dropDuplicates()\n",
    "\n",
    "# COMMAND ----------\n",
    "# --- 5. LOAD TO SILVER LAYER: Write Cleaned Data as Delta Lake ---\n",
    "\n",
    "# Use Delta format for reliability, transactions, and schema enforcement.\n",
    "\n",
    "writer = entity_table.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "\n",
    "writer.save(SILVER_FULL_PATH)\n",
    "\n",
    "print(f\"âœ… Cleaned {entity_type} data successfully saved to Silver layer at: {SILVER_FULL_PATH}\")\n",
    "print(f\"Processed {entity_table.count()} cleaned records\")\n",
    "print(\"Next Steps: Gold layer processing will handle aggregations and joins.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
