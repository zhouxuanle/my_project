## ğŸš€ V1.0 Implementation Guide: Orchestrated Data Cleaning & Loading

### Phase Overview
This document outlines the implementation of **V1.0: Orchestrated Data Cleaning & Loading (Stage 2)** for the Universal AI Data Orchestrator.

**Goal:** Implement a production-ready, dual-path ETL pipeline based on data volume using Azure Data Factory (ADF), Azure Functions, and Azure Databricks.

---

## âœ… Completed Tasks

### 1. Data Routing Logic âœ…
**File:** `backend/data_routing.py`

Implemented `DataRouter` class that routes data based on size:
- **Small Batch** (â‰¤10k records): Routes to `small-batch-queue` for fast path processing
- **Large Batch** (>10k records): Routes to `large-batch-queue` for heavy path processing

Features:
- `route_data_request()`: Determines appropriate path and provides decision metadata
- `queue_message_to_path()`: Sends message to appropriate Azure Queue
- `get_queue_config()`: Returns pipeline configuration for each path
- Comprehensive logging and error handling

### 2. Backend API Endpoints âœ…
**File:** `backend/routes/data.py`

Added three new endpoints:

#### `/clean_data` (POST) - V1.0 Main Endpoint
Initiates data cleaning workflow through ADF
```bash
curl -X POST http://localhost:5000/clean_data \
  -H "Authorization: Bearer <JWT_TOKEN>" \
  -H "Content-Type: application/json" \
  -d '{"dataCount": 5000, "jobName": "Q4_2024_Sales"}'
```

Response:
```json
{
  "success": true,
  "jobId": "uuid",
  "parentJobId": "uuid",
  "processingPath": "small_batch",
  "queueName": "small-batch-queue",
  "expectedProcessingTime": "<5 minutes (10-min ADF trigger)",
  "recordCount": 5000
}
```

#### `/routing_config/<path_type>` (GET) - Configuration Endpoint
Get details about processing path configuration
```bash
curl http://localhost:5000/routing_config/small_batch \
  -H "Authorization: Bearer <JWT_TOKEN>"
```

---

## ğŸ“‹ Tasks In Progress

### 2. Azure Data Factory Pipeline - Medallion Architecture (Bronze â†’ Silver â†’ Gold)

The following needs to be configured in Azure portal:

**Medallion Architecture Design:**
- **Bronze Layer:** Raw data source (`shanlee-raw-data/`) - READ ONLY, no duplication
- **Silver Layer:** Cleaned, standardized data (`silver/cleaned/`) - WRITE transformation output
- **Gold Layer:** Analytics-ready, aggregated data (`gold/analytics/`) - WRITE aggregation output

#### Small Batch Pipeline Configuration

**Pipeline Name:** `SmallBatchCleaningPipeline`

**Trigger:**
- Type: Schedule Trigger
- Frequency: Every 10 minutes
- Start Time: Now

**Activities:**

1. **Queue Listener Activity**
   - Type: Azure Function Activity
   - Function: `process_small_batch_queue`
   - Input: Reads from `small-batch-queue`

2. **Read Bronze Layer**
   - Activity: Azure Function Activity
   - Function: `transform_small_batch_queue`
  - Source: Azure Blob Storage (`shanlee-raw-data/{userId}/{parentJobId}/{jobId}.json`)
   - Operations:
     - Read raw JSON data (no copy, direct read in function)
     - Parse and extract entities
     - Perform data quality checks

3. **Transform to Silver Layer**
   - Activity: Azure Function Activity (internal operation)
   - Function: `PandasTransformer.transform_to_silver()`
   - Operations:
     - Remove duplicates (by user_id)
     - Standardize date formats (ISO 8601)
     - Validate and normalize email/phone
     - Handle missing values
  - Output: Parquet files to `silver/cleaned/{userId}/{parentJobId}/{jobId}.parquet`

4. **Transform to Gold Layer**
   - Activity: Azure Function Activity (internal operation)
   - Function: `PandasTransformer.transform_to_gold()`
   - Operations:
     - Create dimensional tables:
       - `dim_users` - User dimension
       - `dim_location` - Location/address dimension
       - `dim_time` - Time dimension
       - `dim_product` - Product dimension
     - Create fact table:
       - `fact_orders` - Order transactions
     - Create aggregates:
       - `agg_user_metrics` - User-level aggregations
  - Output: Parquet files to `gold/analytics/{userId}/{parentJobId}/{jobId}/*.parquet`

5. **Load to Synapse**
   - Activity: Copy Data
   - Source: Gold Layer (ADLS Gen2 - `gold/analytics/`)
   - Sink: Azure Synapse Analytics
   - Tables: `dw_users`, `dw_locations`, `dw_products`, `dw_orders`, `dw_user_metrics`

6. **Load to MySQL**
   - Activity: Copy Data
   - Source: Gold Layer (ADLS Gen2 - `gold/analytics/`)
   - Sink: Azure MySQL (backend database)
   - Tables: Same as Synapse (for operational queries)
   - Insert Mode: Upsert (by primary key)

#### Large Batch Pipeline Configuration

**Pipeline Name:** `LargeBatchCleaningPipeline`

**Trigger:**
- Type: Schedule Trigger
- Frequency: Every 10 hours (UTC)
- Start Time: Now

**Activities (queue-aware fan-out):**

1. **Queue Fetch Activity**
   - Type: Azure Function or HTTP activity
   - Purpose: Peek `large-batch-queue` and return pending jobs (userId, parentJobId, jobId, timestamp)
   - Behavior: If no jobs are returned, pipeline ends

2. **ForEach Pending Job**
   - Concurrency: 3â€“5 (tune per cluster size)
   - Items: Jobs returned from step 1

   **Inside ForEach:**
   - **Read Bronze + Bronze â†’ Silver (Databricks Notebook)**
     - Notebook: `/Shared/Notebooks/bronze_to_silver_pyspark`
     - Source: `shanlee-raw-data/{userId}/{parentJobId}/{jobId}.json`
     - Ops: distributed read, validation, dedupe, standardize dates/phones/emails, fill missing
     - Output: `silver/cleaned/{userId}/{parentJobId}/{jobId}.parquet`

   - **Silver â†’ Gold (Databricks Notebook)**
     - Notebook: `/Shared/Notebooks/silver_to_gold_pyspark`
     - Ops: build dims (`dim_users`, `dim_location`, `dim_time`, `dim_product`), facts (`fact_orders`), aggs (`agg_user_metrics`, `agg_category_metrics`)
     - Output: `gold/analytics/{userId}/{parentJobId}/{jobId}/*.parquet`

   - **Optional: Load/Notify**
     - Copy Data to Synapse and MySQL (bulk upsert)
     - Emit metrics/SignalR notification

---

## ğŸ”§ Required Azure Resources

### Queues
```
âœ“ small-batch-queue (created automatically by code)
âœ“ large-batch-queue (created automatically by code)
```

### Storage Accounts
```
âœ“ Existing: Azure Blob Storage (shanlee-raw-data container) - Bronze Layer
  - Purpose: Raw data source (READ ONLY)
  - No duplication needed - already populated by generation step
  
âœ“ Needed: ADLS Gen2 for Silver and Gold layers
  - Containers:
    - silver/cleaned/{userId}/{parentJobId}/{jobId}.parquet
    - gold/analytics/{userId}/{parentJobId}/{jobId}/*.parquet
```

### Databases
```
âœ“ Azure MySQL (operational database)
âœ“ Azure Synapse Analytics (data warehouse) - NEW
  - Tables:
    - dw_ecommerce_gold (fact/dimension tables)
```

### Compute
```
âœ“ Azure Functions (Python 3.10+)
âœ“ Azure Databricks (Standard cluster)
```

---

## ğŸ“ Next Steps (Task 3-6)

### Task 3: Transformation Functions
Files to create:
- `backend/transformations/pandas_transforms.py` - Silver & Gold layer for small batches
- `backend/transformations/pyspark_transforms.py` - Notebook scripts for Databricks

### Task 4: ADF Pipeline Configuration
- Create pipelines in Azure Portal
- Configure linked services (Synapse, MySQL, Databricks)
- Set up scheduling and triggers
- Configure error handling and retry policies

### Task 5: SignalR Integration
Update `backend/routes/jobs.py`:
- Hook ADF pipeline status to SignalR notifications
- Track Bronze (READ) â†’ Silver (WRITE) â†’ Gold (WRITE) progress
- Send real-time updates to frontend
- Monitor transformation metrics

### Task 6: Validation (tests deferred)
- Test execution deferred per request

---

## ğŸ”— Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Flask Backend (V1.0)                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  /clean_data endpoint                                      â”‚
â”‚  â”œâ”€> DataRouter (routing_logic.py)                        â”‚
â”‚  â””â”€> Sends to appropriate Azure Queue                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚                       â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Small Batch   â”‚      â”‚  Large Batch    â”‚
        â”‚  (â‰¤10k recs)   â”‚      â”‚  (>10k recs)    â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚                       â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”
        â”‚  Azure Data Factory Pipelines        â”‚
        â”‚  â”œâ”€ 10-min trigger (small)          â”‚
        â”‚  â””â”€ 10-hour trigger (large, queue check) â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
                â”‚                        â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
        â”‚ Azure Func   â”‚        â”‚  Databricks   â”‚
        â”‚ (Pandas)     â”‚        â”‚  (PySpark)    â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                â”‚                        â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”
        â”‚   Medallion Architecture Layers      â”‚
        â”‚   â”œâ”€ Bronze: shanlee-raw-data (READ) â”‚
        â”‚   â”œâ”€ Silver: silver/cleaned (WRITE)  â”‚
        â”‚   â””â”€ Gold: gold/analytics (WRITE)    â”‚
        â”‚   Note: No Bronze duplication        â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
                â”‚                        â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
        â”‚  Azure       â”‚        â”‚  Azure MySQL  â”‚
        â”‚  Synapse     â”‚        â”‚  Database     â”‚
        â”‚  Analytics   â”‚        â”‚  (Operational)â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ§ª Validation (tests deferred)
- Formal test scenarios deferred per request.
- [ ] SignalR sends status updates

---

## ğŸ“Š Monitoring & Observability

### Key Metrics to Track
1. Queue message count per queue
2. ADF pipeline execution time (Bronze â†’ Silver â†’ Gold)
3. Load success rate (records loaded vs. total)
4. End-to-end latency (submission â†’ completion)

### Logs to Monitor
- Azure Functions: `process_data_generation_job`
- ADF Pipeline Runs: Activity failures, duration
- Databricks: Cluster startup time, transformation duration
- SignalR: Notification delivery

---

## ğŸ’¡ Design Decisions

### Why Medallion Architecture?
- **Separation of Concerns:** Raw â†’ Cleaned â†’ Analytics
- **Data Lineage:** Easy to trace data transformations
- **Reusability:** Silver layer can feed multiple Gold tables
- **Cost Optimization:** Process only what's needed per layer

### Why Dual-Path Processing?
- **Small Batches:** Quick response time with Pandas (high concurrency)
- **Large Batches:** Better resource efficiency with Spark (parallel processing)
- **Cost Efficiency:** Right tool for the job prevents overprovisioning

### Why Azure Synapse + MySQL?
- **Synapse:** OLAP (Analytics & Reporting)
- **MySQL:** OLTP (Operational queries & transactions)

---

## ğŸ”’ Security Considerations

- [ ] Enable Managed Identity for ADF
- [ ] Use Azure Key Vault for connection strings
- [ ] Encrypt data in transit (TLS 1.2+)
- [ ] Encrypt data at rest (ADLS Gen2, Synapse)
- [ ] Implement row-level security in Synapse
- [ ] Audit all data access via Azure Monitor

---

## ğŸ“– References

- [Azure Data Factory Documentation](https://learn.microsoft.com/en-us/azure/data-factory/)
- [Medallion Architecture](https://www.databricks.com/blog/2022/06/24/use-the-medallion-architecture-to-build-elt-pipelines-using-apache-spark.html)
- [Azure Databricks SQL](https://docs.databricks.com/)
- [Azure Synapse Best Practices](https://learn.microsoft.com/en-us/azure/synapse-analytics/)

