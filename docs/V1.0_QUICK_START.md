# âš¡ V1.0 Quick Start Guide

Get Phase 1 up and running in 30 minutes.

---

## 5-Minute Setup Overview

### What You'll Have After This Guide
âœ… All core code implemented  
âœ… Routing logic ready  
âœ… API endpoints functional  
âœ… Transformation pipelines configured  
âœ… Documentation complete  

### What You Still Need to Do
â³ Deploy Azure infrastructure  
â³ Configure Azure Data Factory pipelines  
â³ Test in Azure environment  

---

## Step 1: Verify Code Installation (5 min)

### Backend Files Created
```bash
# Verify all new files exist
ls -la backend/data_routing.py
ls -la backend/transformations/
ls -la backend/myfunc/functions/small_batch_functions.py
ls -la backend/routes/data.py

# Should see:
# âœ“ data_routing.py (350+ lines)
# âœ“ transformations/__init__.py
# âœ“ transformations/pandas_transforms.py (400+ lines)
# âœ“ transformations/pyspark_transforms.py (600+ lines)
# âœ“ small_batch_functions.py (300+ lines)
```

### Verify Updated Files
```bash
# Check imports added to routes/data.py
grep "from data_routing import" backend/routes/data.py
grep "/clean_data" backend/routes/data.py

# Check transformations imported
grep "from transformations import" backend/routes/data.py
```

### Verify Requirements Updated
```bash
# Check new packages in requirements
grep pandas backend/requirements.txt
grep azure-storage-queue backend/requirements.txt
grep azure-data-tables backend/requirements.txt
```

---

## Step 2: Install Dependencies (3 min)

> Use the correct virtual environment per OS: **my_env_mac** on macOS, **my_env** on Windows/PC.

```bash
cd /Users/lee/Documents/my_project

# macOS: activate my_env_mac
source my_env_mac/bin/activate

# Windows/PC: activate my_env
my_env\Scripts\activate

# Install updated requirements (after activation)
pip install -r backend/requirements.txt

# Verify installations
python -c "from data_routing import DataRouter; print('âœ“ data_routing OK')"
python -c "from transformations import PandasTransformer; print('âœ“ transformations OK')"
python -c "import pandas; print(f'âœ“ pandas {pandas.__version__} OK')"
```

---

## Step 3: Test Core Logic Locally (5 min)

### Test Data Routing

```bash
python << 'EOF'
from backend.data_routing import DataRouter, ProcessingPath

router = DataRouter()

# Validate small batch
decision_small = router.route_data_request(
    user_id="test_user",
    count=5000,
    job_id="test_job_1"
)
print(f"Small batch (5000): {decision_small.path.value}")
assert decision_small.path == ProcessingPath.SMALL_BATCH

# Validate large batch
decision_large = router.route_data_request(
    user_id="test_user",
    count=50000,
    job_id="test_job_2"
)
print(f"Large batch (50000): {decision_large.path.value}")
assert decision_large.path == ProcessingPath.LARGE_BATCH

print("âœ“ Routing validation passed!")
EOF
```

### Validate Transformation Logic

```bash
python << 'EOF'
from backend.transformations import PandasTransformer
import json

# Sample data
sample_data = [{
    "user": {
        "id": "u1",
        "username": "john_doe",
        "email": "john@example.com",
        "phone_number": "1234567890",
        "age": 30,
        "create_time": "2024-01-01T00:00:00Z"
    },
    "address": {
        "id": "a1",
        "user_id": "u1",
        "city": "new york",
        "country": "usa",
        "postal_code": "10001",
        "create_time": "2024-01-01T00:00:00Z"
    },
    "product": {
        "id": "p1",
        "name": "Widget",
        "category_id": "c1",
        "create_time": "2024-01-01T00:00:00Z"
    },
    "order": {
        "id": "o1",
        "user_id": "u1",
        "create_time": "2024-01-01T00:00:00Z"
    }
}]

# Test Silver transformation
transformer = PandasTransformer()
silver_df, metadata = transformer.transform_to_silver(sample_data)

print(f"âœ“ Silver transformation successful")
print(f"  Records processed: {metadata['total_records_processed']}")
print(f"  Records cleaned: {metadata['total_records_cleaned']}")
print(f"  Quality score: {metadata['quality_score_avg']:.2f}")

# Test Gold transformation
gold_tables, gold_metadata = transformer.transform_to_gold(silver_df)
print(f"âœ“ Gold transformation successful")
print(f"  Tables created: {list(gold_tables.keys())}")
EOF
```

---

## Step 4: Start Flask Backend (5 min)

```bash
# Terminal 1: Start Flask development server
cd /Users/lee/Documents/my_project/backend
python app.py

# Should see:
# Running on http://127.0.0.1:5000
# WARNING: This is a development server...
```

### Test Health Check

```bash
# Terminal 2: Test endpoints
curl http://localhost:5000/

# You should get a response (may be 404 if no root endpoint, that's OK)
```

---

## Step 5: Test Routing Endpoints (5 min)

### Get JWT Token (using existing auth)

```bash
# First, login to get token
TOKEN=$(curl -s -X POST http://localhost:5000/login \
  -H "Content-Type: application/json" \
  -d '{"username": "testuser", "password": "testpass"}' \
  | jq -r '.access_token')

echo "Token: $TOKEN"
```

### Test /clean_data Endpoint

```bash
# Submit small batch job
curl -X POST http://localhost:5000/clean_data \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"dataCount": 5000, "jobName": "quick_test"}' \
  | jq

# Expected response:
# {
#   "success": true,
#   "jobId": "uuid",
#   "processingPath": "small_batch",
#   "queueName": "small-batch-queue",
#   ...
# }
```

### Validate /routing_config Endpoint

```bash
# Get small batch config
curl http://localhost:5000/routing_config/small_batch \
  -H "Authorization: Bearer $TOKEN" \
  | jq '.config'

# Get large batch config
curl http://localhost:5000/routing_config/large_batch \
  -H "Authorization: Bearer $TOKEN" \
  | jq '.config'
```

---

## Step 6: Review Documentation (2 min)

Before Azure deployment, review these files in order:

1. **V1.0_SUMMARY.md** (2 min) - This quick overview
2. **V1.0_IMPLEMENTATION.md** (10 min) - Detailed architecture
3. **V1.0_API.md** (5 min) - API specification
```bash
# View documentation
open docs/V1.0_SUMMARY.md
open docs/V1.0_IMPLEMENTATION.md
open docs/V1.0_API.md
```

---

## Next: Azure Deployment Steps

### When Ready for Cloud:

1. **Set Up Azure Infrastructure** (15 min)
   ```bash
   chmod +x scripts/setup_v1_infrastructure.sh
   ./scripts/setup_v1_infrastructure.sh
   ```

2. **Configure Azure Data Factory** (30 min)
   - Create ADF instance
   - Import pipelines (use v1_adf_template.json)
   - Configure linked services
   - Set up triggers

3. **Deploy Azure Functions** (20 min)
   - Package small_batch_functions.py
   - Deploy to Azure Functions
   - Test with sample data

4. **Configure Databricks** (30 min)
   - Create workspace and cluster
   - Import transformation notebooks
   - Configure ADLS mounts
   - Test notebook execution

5. **Validation:** Formal testing deferred per request; perform targeted manual checks when ready

---

## Architecture at a Glance

```
User Request
  â†“
/clean_data endpoint
  â†“
DataRouter (â‰¤10k? â†’ small-batch : large-batch)
  â†“
Azure Queue Storage
  â†“
ADF Pipeline (10-min small, 10-hour large with queue check)
  â†“
Azure Function / Databricks
  â†“
Medallion Layers (Bronze â†’ Silver â†’ Gold)
  â†“
Azure Synapse + MySQL
```

---

## File Structure Overview

```
/Users/lee/Documents/my_project/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ data_routing.py          â† NEW: Routing logic
â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â””â”€â”€ data.py              â† UPDATED: New endpoints
â”‚   â”œâ”€â”€ transformations/          â† NEW: Data transformation
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ pandas_transforms.py
â”‚   â”‚   â””â”€â”€ pyspark_transforms.py
â”‚   â”œâ”€â”€ myfunc/
â”‚   â”‚   â”œâ”€â”€ functions/
â”‚   â”‚   â”‚   â””â”€â”€ small_batch_functions.py â† NEW
â”‚   â”‚   â””â”€â”€ function_app.py       â† UPDATED: New function registrations
â”‚   â””â”€â”€ requirements.txt          â† UPDATED: New packages
â”œâ”€â”€ infrastructure/
â”‚   â””â”€â”€ v1_adf_template.json     â† NEW: ARM template
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ setup_v1_infrastructure.sh â† NEW: Setup script
â””â”€â”€ docs/
    â”œâ”€â”€ V1.0_SUMMARY.md          â† NEW: This quick start
    â”œâ”€â”€ V1.0_IMPLEMENTATION.md   â† NEW: Full details
    â”œâ”€â”€ V1.0_API.md              â† NEW: API spec
```

---

## Troubleshooting Quick Fixes

### Issue: Import errors
```bash
# Reinstall requirements
pip install -r backend/requirements.txt --force-reinstall
```

### Issue: JWT token errors
```bash
# Make sure you're including the Bearer prefix
curl -H "Authorization: Bearer $TOKEN" ...  # âœ“ Correct
curl -H "Authorization: $TOKEN" ...          # âœ— Wrong
```

### Issue: Module not found
```bash
# Ensure you're in correct directory
cd /Users/lee/Documents/my_project
python -c "from backend.data_routing import DataRouter; print('OK')"
```

### Issue: Port 5000 already in use
```bash
# Find and kill process on port 5000
lsof -ti :5000 | xargs kill -9
python app.py  # Try again
```

---

## Success Checklist âœ…

After following this guide, you should have:

- [ ] All files created and verified
- [ ] Dependencies installed
- [ ] Routing logic tested locally
- [ ] Transformation functions tested
- [ ] Flask server running
- [ ] API endpoints responding
- [ ] Documentation reviewed
- [ ] Ready for Azure deployment

---

## What's Working Now

âœ… **Code Level:**
- Size-based routing logic
- Pandas transformations
- PySpark templates
- API endpoints
- Azure Functions framework

â³ **Cloud Level (requires Azure deployment):**
- Azure Data Factory pipelines
- Azure Functions execution
- Databricks processing
- ADLS Gen2 storage
- Synapse loading
- MySQL loading

---

## Next Steps

1. **Review Documentation** â†’ Start with V1.0_IMPLEMENTATION.md
2. **Deploy to Azure** â†’ Run setup_v1_infrastructure.sh
3. **Configure ADF** â†’ Follow ARM template guide
4. **Go Live!** â†’ Start processing data (validation deferred)

---

## Key Commands Summary

```bash
# Install & validate
source my_env_mac/bin/activate   # macOS
my_env\Scripts\activate         # Windows/PC
pip install -r backend/requirements.txt
python -c "from backend.data_routing import DataRouter; print('âœ“')"

# Start server
cd backend && python app.py

# Test endpoints
TOKEN="your_jwt_token"
curl -X POST http://localhost:5000/clean_data \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"dataCount": 5000}'

# View docs
open docs/V1.0_*.md

# Azure setup (when ready)
chmod +x scripts/setup_v1_infrastructure.sh
./scripts/setup_v1_infrastructure.sh
```

---

## Support Resources

- **Implementation Details:** docs/V1.0_IMPLEMENTATION.md
- **API Reference:** docs/V1.0_API.md
- **Project Roadmap:** ROADMAP.md
- **Azure Data Factory:** https://aka.ms/ADF
- **Databricks:** https://databricks.com/product

---

**ðŸŽ‰ You're all set! Phase 1 (V1.0) core implementation is complete.**

**Time to deployment: ~2 hours for full Azure setup**

