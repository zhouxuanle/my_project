# V1.0 API Documentation

Complete API specification for Phase 1 implementation.

## Overview

V1.0 introduces the `/clean_data` endpoint which initiates data cleaning workflows through Azure Data Factory. The endpoint automatically routes requests to the appropriate processing path based on data volume.


## Endpoints

### 1. Clean Data (Primary V1.0 Endpoint)

**Endpoint:** `POST /clean_data`

**Authentication:** Required (JWT Bearer Token)

**Description:** Submit data for cleaning and loading via dual-path ETL pipeline.

#### Request

```bash
curl -X POST http://localhost:5000/clean_data \
  -H "Authorization: Bearer <JWT_TOKEN>" \
  -H "Content-Type: application/json" \
  -d '{
    "dataCount": 5000,
    "jobName": "Q4_2024_Sales_Analysis"
  }'
```

**Request Body Schema:**

```json
{
  "dataCount": {
    "type": "integer",
    "description": "Number of e-commerce records to generate and clean",
    "minimum": 1,
    "maximum": null,
    "required": true
  },
  "jobName": {
    "type": "string",
    "description": "Optional descriptive name for the job",
    "required": false,
    "default": "clean_job_{random_hex}"
  }
}
```

#### Response - Success (200 OK)

```json
{
  "success": true,
  "jobId": "550e8400-e29b-41d4-a716-446655440001",
  "parentJobId": "550e8400-e29b-41d4-a716-446655440002",
  "processingPath": "small_batch",
  "queueName": "small-batch-queue",
  "expectedProcessingTime": "<5 minutes (10-min ADF trigger)",
  "recordCount": 5000,
  "jobName": "Q4_2024_Sales_Analysis",
  "message": "Data cleaning job queued to small-batch-queue for 5000 records"
}
```

**Response Fields:**

| Field | Type | Description |
|-------|------|-------------|
| `success` | boolean | Request successful |
| `jobId` | string (UUID) | Unique identifier for this cleaning job |
| `parentJobId` | string (UUID) | Parent job ID (same as jobId for non-chunked) |
| `processingPath` | string | `small_batch` or `large_batch` |
| `queueName` | string | Azure Queue name where job was queued |
| `expectedProcessingTime` | string | Estimated time to completion |
| `recordCount` | integer | Number of records being processed |
| `jobName` | string | Descriptive job name |
| `message` | string | Human-readable status message |

#### Response - Error (400 Bad Request)

Invalid input parameters:

```json
{
  "success": false,
  "message": "dataCount must be > 0"
}
```

#### Response - Error (500 Internal Server Error)

Server-side processing failure:

```json
{
  "success": false,
  "message": "Failed to route data for cleaning: {error_details}"
}
```

#### Processing Paths

**Small Batch** (≤10k records):

**Large Batch** (>10k records):

#### Examples

**Small Batch Request:**

```bash
curl -X POST http://localhost:5000/clean_data \
  -H "Authorization: Bearer eyJhbGciOiJIUzI1NiIs..." \
  -H "Content-Type: application/json" \
  -d '{
    "dataCount": 5000,
    "jobName": "Daily_Clean_Job"
  }'
```

**Response:**

```json
{
  "success": true,
  "jobId": "a1b2c3d4-e5f6-47g8-h9i0-j1k2l3m4n5o6",
  "parentJobId": "a1b2c3d4-e5f6-47g8-h9i0-j1k2l3m4n5o6",
  "processingPath": "small_batch",
  "queueName": "small-batch-queue",
  "expectedProcessingTime": "<5 minutes (10-min ADF trigger)",
  "recordCount": 5000,
  "jobName": "Daily_Clean_Job",
  "message": "Data cleaning job queued to small-batch-queue for 5000 records"
}
```

**Large Batch Request:**

```bash
curl -X POST http://localhost:5000/clean_data \
  -H "Authorization: Bearer eyJhbGciOiJIUzI1NiIs..." \
  -H "Content-Type: application/json" \
  -d '{
    "dataCount": 50000,
    "jobName": "Monthly_Archive_Processing"
  }'
```

**Response:**

```json
{
  "success": true,
  "jobId": "b2c3d4e5-f6g7-48h9-i0j1-k2l3m4n5o6p7",
  "parentJobId": "b2c3d4e5-f6g7-48h9-i0j1-k2l3m4n5o6p7",
  "processingPath": "large_batch",
  "queueName": "large-batch-queue",
  "expectedProcessingTime": "<24 hours (daily ADF trigger)",
  "recordCount": 50000,
  "jobName": "Monthly_Archive_Processing",
  "message": "Data cleaning job queued to large-batch-queue for 50000 records"
}
```


### 2. Routing Configuration

**Endpoint:** `GET /routing_config/<path_type>`

**Authentication:** Required (JWT Bearer Token)

**Description:** Get detailed configuration for a specific processing path.

#### Request

```bash
curl http://localhost:5000/routing_config/small_batch \
  -H "Authorization: Bearer <JWT_TOKEN>"
```

**Path Parameters:**

| Parameter | Type | Values |
|-----------|------|--------|
| `path_type` | string | `small_batch` or `large_batch` |

#### Response - Success (200 OK)

```json
{
  "success": true,
  "pathType": "small_batch",
  "config": {
    "queue_name": "small-batch-queue",
    "adf_trigger_type": "ScheduleTrigger",
    "adf_trigger_interval": 10,
    "adf_trigger_frequency": "Minute",
    "processor": "Azure Function (Pandas)",
    "expected_throughput": "~5-10k records per batch",
    "max_concurrent_jobs": 10,
    "storage_account_tier": "Standard"
  }
}
```

#### Response - Error (400 Bad Request)

```json
{
  "success": false,
  "message": "Invalid path_type: invalid. Use \"small_batch\" or \"large_batch\""
}
```

#### Examples

**Small Batch Configuration:**

```bash
curl http://localhost:5000/routing_config/small_batch \
  -H "Authorization: Bearer eyJhbGciOiJIUzI1NiIs..."
```

**Response:**

```json
{
  "success": true,
  "pathType": "small_batch",
  "config": {
    "queue_name": "small-batch-queue",
    "adf_trigger_type": "ScheduleTrigger",
    "adf_trigger_interval": 10,
    "adf_trigger_frequency": "Minute",
    "processor": "Azure Function (Pandas)",
    "expected_throughput": "~5-10k records per batch",
    "max_concurrent_jobs": 10,
    "storage_account_tier": "Standard"
  }
}
```

**Large Batch Configuration:**

```bash
curl http://localhost:5000/routing_config/large_batch \
  -H "Authorization: Bearer eyJhbGciOiJIUzI1NiIs..."
```

**Response:**

```json
{
  "success": true,
  "pathType": "large_batch",
  "config": {
    "queue_name": "large-batch-queue",
    "adf_trigger_type": "ScheduleTrigger",
    "adf_trigger_interval": 1,
    "adf_trigger_frequency": "Day",
    "processor": "Azure Databricks (PySpark)",
    "expected_throughput": "100k+ records per batch",
    "max_concurrent_jobs": 3,
    "storage_account_tier": "Premium"
  }
}
```


## Status Codes

| Code | Name | Description |
|------|------|-------------|
| 200 | OK | Request successful |
| 400 | Bad Request | Invalid parameters or missing required fields |
| 401 | Unauthorized | Missing or invalid JWT token |
| 403 | Forbidden | User not authorized for this operation |
| 500 | Internal Server Error | Server-side processing error |


## Error Handling

All error responses follow this schema:

```json
{
  "success": false,
  "message": "Human-readable error description"
}
```

**Common Errors:**

| Error | Cause | Solution |
|-------|-------|----------|
| `dataCount must be > 0` | Invalid record count | Provide count ≥ 1 |
| `Missing required fields` | Incomplete request body | Include required fields |
| `Failed to route data` | Backend routing failure | Check logs, retry |
| `Missing or expired token` | Auth failure | Provide valid JWT token |


## Authentication

All endpoints require JWT Bearer token authentication.

**Header Format:**
```
Authorization: Bearer <JWT_TOKEN>
```

**Example:**
```bash
curl -H "Authorization: Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9..." \
  http://localhost:5000/clean_data
```


## Rate Limiting

Current implementation has no rate limits, but future versions may implement:



## Data Flow

```
Client Request (/clean_data)
      │
      ▼
Flask /clean_data Endpoint
      │
      ▼
DataRouter.route_data_request()
      │
      ├─► Small Batch? (count ≤ 10k)
      │   │
      │   ▼
      │   Queue → small-batch-queue
      │   │
      │   ▼
      │   ADF (10-min trigger)
      │   │
      │   ▼
      │   Azure Function (Pandas)
      │   │
      │   ├─► READ: Bronze (shanlee-raw-data/{userId}/{parentJobId}/{jobId}.json)
      │   │
      │   ├─► TRANSFORM: Clean & Standardize (Silver layer)
      │   │
      │   ├─► WRITE: Silver (silver/cleaned/{userId}/{parentJobId}/{jobId}.parquet)
      │   │
      │   └─► WRITE: Gold (gold/analytics/{userId}/{parentJobId}/{jobId}/*.parquet)
      │
        └─► Large Batch? (count > 10k)
          │
          ▼
          Queue → large-batch-queue
          │
          ▼
          ADF (10-hour schedule + queue check)
          │
          ▼
          Databricks (PySpark)
          │
          ├─► READ: Bronze (shanlee-raw-data/{userId}/{parentJobId}/{jobId}.json)
          │
          ├─► TRANSFORM: Distributed clean (Silver layer)
          │
          ├─► WRITE: Silver (silver/cleaned/{userId}/{parentJobId}/{jobId}.parquet)
          │
          └─► WRITE: Gold (gold/analytics/{userId}/{parentJobId}/{jobId}/*.parquet)
                 │
                 ▼
          Load to Analytics:
          - Azure Synapse Analytics
          - Azure MySQL (operational DB)
```

**Architecture Notes:**


## Response Time SLA

| Operation | Target | Actual* |
|-----------|--------|---------|
| Route decision | < 100ms | ~50ms |
| Queue insertion | < 500ms | ~200ms |
| API response | < 1s | ~500ms |
| Pandas transformation | < 30s | ~15s (for 10k) |
| PySpark transformation | < 900s | ~600s (for 50k) |
| Total end-to-end (small) | < 5 min | ~4 min |
| Total end-to-end (large) | < 24h | ~20h |

*Varies based on data complexity and system load


## Request/Response Examples

### cURL Examples

**Submit Small Batch:**
```bash
curl -X POST http://localhost:5000/clean_data \
  -H "Authorization: Bearer YOUR_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"dataCount": 5000}' | jq
```

**Get Configuration:**
```bash
curl http://localhost:5000/routing_config/small_batch \
  -H "Authorization: Bearer YOUR_TOKEN" | jq
```

### Python Examples

```python
import requests
import json

BASE_URL = "http://localhost:5000"
HEADERS = {
    "Authorization": f"Bearer {jwt_token}",
    "Content-Type": "application/json"
}

# Submit cleaning job
response = requests.post(
    f"{BASE_URL}/clean_data",
    headers=HEADERS,
    json={"dataCount": 5000, "jobName": "test_job"}
)
result = response.json()
print(f"Job ID: {result['jobId']}")
print(f"Path: {result['processingPath']}")
print(f"Queue: {result['queueName']}")

# Get config
config_response = requests.get(
    f"{BASE_URL}/routing_config/small_batch",
    headers=HEADERS
)
config = config_response.json()
print(f"Trigger Frequency: {config['config']['adf_trigger_frequency']}")
```

### JavaScript/Node.js Examples

```javascript
const axios = require('axios');

const baseURL = 'http://localhost:5000';
const token = 'YOUR_JWT_TOKEN';

// Submit cleaning job
async function submitCleaningJob(dataCount) {
  try {
    const response = await axios.post(
      `${baseURL}/clean_data`,
      { dataCount, jobName: 'test_job' },
      { headers: { 'Authorization': `Bearer ${token}` } }
    );
    console.log('Job submitted:', response.data);
    return response.data;
  } catch (error) {
    console.error('Error:', error.response.data);
  }
}

// Get routing config
async function getRoutingConfig(pathType) {
  try {
    const response = await axios.get(
      `${baseURL}/routing_config/${pathType}`,
      { headers: { 'Authorization': `Bearer ${token}` } }
    );
    console.log('Config:', response.data);
    return response.data;
  } catch (error) {
    console.error('Error:', error.response.data);
  }
}

submitCleaningJob(5000);
getRoutingConfig('small_batch');
```


## Migration Notes

**From Existing API:**

The `/write_to_db` endpoint still exists for backward compatibility but is superseded by `/clean_data` for orchestrated processing.



## Future Enhancements (V2.0+)


